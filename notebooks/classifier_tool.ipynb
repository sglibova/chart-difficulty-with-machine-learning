{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, the intent is to classify a song using its data as \"stamina\" or \"tech\" to add a feature to the app which removes user input and instead detects which rating model should be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import plot_confusion_matrix, precision_score, recall_score, accuracy_score, f1_score,\\\n",
    "                            roc_curve, auc, classification_report, log_loss, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from statsmodels.api import qqplot\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor, plot_importance, plot_tree, DMatrix\n",
    "import shap\n",
    "\n",
    "from step_parser import batch_analysis\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##display all dataframe columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "stam = pd.read_csv(\"../data/stam.csv\")\n",
    "tech = pd.read_csv(\"../data/not_stam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.data_cleaning import data_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Tech DataFrame with 4808 rows.\n",
      "\n",
      "Removing outliers and filling NaN values...\n",
      "\n",
      "The songs in this Tech dataset are up to 164.002 seconds (2.733 minutes) long.\n",
      "The songs in this Tech dataset have up to 826 steps.\n",
      "The songs in this Tech dataset have a max  bpm weighted average up to 175 bpm.\n",
      "The songs in this Tech dataset have up to 21 bpm changes.\n",
      "The songs in this Tech dataset have up to 6.387 NPS.\n",
      "\n",
      "Returning cleaned Tech DataFrame with 3209 rows and 31 columns.\n"
     ]
    }
   ],
   "source": [
    "cleaned_tech = data_cleaner(tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Stamina DataFrame with 2926 rows.\n",
      "\n",
      "Removing outliers and filling NaN values...\n",
      "\n",
      "The songs in this Stamina dataset are up to 1695.086 seconds (28.251 minutes) long.\n",
      "The songs in this Stamina dataset have up to 16804steps.\n",
      "The songs in this Stamina dataset have a max  bpm weighted average up to 216 bpm.\n",
      "The songs in this Stamina dataset have up to 4 bpm changes.\n",
      "The songs in this Stamina dataset have up to 10.866 NPS.\n",
      "\n",
      "Returning cleaned Stamina DataFrame with 2255 rows and 40 columns.\n"
     ]
    }
   ],
   "source": [
    "cleaned_stam = data_cleaner(stam, is_stamina = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tech['target'] = 0\n",
    "cleaned_stam['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_songs = pd.concat([cleaned_tech, cleaned_stam], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#due to some columns being removed from the tech dataset for being irrelevant, I will fill the NaN values with 0\n",
    "\n",
    "all_songs = all_songs.fillna(0)\n",
    "\n",
    "#shuffling the dataframe to mix the target values\n",
    "\n",
    "all_songs = all_songs.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_songs.drop(columns=['target', 'title', 'artist', 'difficulty'], axis=1)\n",
    "target = all_songs.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444    0\n",
       "4907    1\n",
       "2320    1\n",
       "2596    1\n",
       "3481    1\n",
       "       ..\n",
       "3236    0\n",
       "4100    1\n",
       "3350    1\n",
       "243     1\n",
       "4681    1\n",
       "Name: target, Length: 1025, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pipe = Pipeline([('scaler', StandardScaler()), ('classifier', XGBClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipe.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = classifier_pipe.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_preds != y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = classifier_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_predictions != y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 .sm files. Running analysis. '.'=success, 'X'=failure, '0'=no singles stepchart\n",
      "..................................................\n",
      "...........................\n",
      "Analysis complete!\n",
      "Merging dataframes\n"
     ]
    }
   ],
   "source": [
    "test_stam = batch_analysis(\"../data/gcs2_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 .sm files. Running analysis. '.'=success, 'X'=failure, '0'=no singles stepchart\n",
      ".........\n",
      "Analysis complete!\n",
      "Merging dataframes\n"
     ]
    }
   ],
   "source": [
    "test_tech = batch_analysis(\"../data/s_selections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stam['rating'] = test_stam.rating.astype(float)\n",
    "test_tech['rating'] = test_tech.rating.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Stamina DataFrame with 77 rows.\n",
      "\n",
      "Removing outliers and filling NaN values...\n",
      "\n",
      "The songs in this Stamina dataset are up to 1095.038 seconds (18.251 minutes) long.\n",
      "The songs in this Stamina dataset have up to 8310steps.\n",
      "The songs in this Stamina dataset have a max  bpm weighted average up to 198 bpm.\n",
      "The songs in this Stamina dataset have up to 5 bpm changes.\n",
      "The songs in this Stamina dataset have up to 10.125 NPS.\n",
      "\n",
      "Returning cleaned Stamina DataFrame with 54 rows and 40 columns.\n",
      "Initialized Tech DataFrame with 17 rows.\n",
      "\n",
      "Removing outliers and filling NaN values...\n",
      "\n",
      "The songs in this Tech dataset are up to 158.354 seconds (2.639 minutes) long.\n",
      "The songs in this Tech dataset have up to 890 steps.\n",
      "The songs in this Tech dataset have a max  bpm weighted average up to 186 bpm.\n",
      "The songs in this Tech dataset have up to 1 bpm changes.\n",
      "The songs in this Tech dataset have up to 6.307 NPS.\n",
      "\n",
      "Returning cleaned Tech DataFrame with 9 rows and 31 columns.\n"
     ]
    }
   ],
   "source": [
    "test_stam = data_cleaner(test_stam, is_stamina=True)\n",
    "test_tech = data_cleaner(test_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tech['target'] = 0\n",
    "test_stam['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier_set = pd.concat([test_tech, test_stam], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier_set = verifier_set.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier_set = verifier_set.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = verifier_set.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_titles = testing_set[['title', 'artist', 'difficulty']]\n",
    "testing_set = testing_set.drop(columns=['title', 'artist', 'difficulty'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = classifier_pipe.predict(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(verifier_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_true != test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
